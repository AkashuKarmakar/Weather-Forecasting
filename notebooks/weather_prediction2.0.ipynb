{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0588cb2b-fb85-4b5b-88b2-52628ef46f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 0: IMPORTS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "import shap\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a66eeae2-7c3b-4a8c-be27-5d399db6e5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: LOAD DATA & FEATURE ENGINEERING\n",
    "df = pd.read_csv(\"F:/projects/personal project/weather prediction_v1/data/weather_atmospheric_processed.csv\", parse_dates=['time'], index_col='time')\n",
    "\n",
    "# Create lag features\n",
    "for col in df.columns:\n",
    "    df[f\"{col}_lag1\"] = df[col].shift(1)\n",
    "\n",
    "# Rolling means\n",
    "for col in df.columns:\n",
    "    df[f\"{col}_roll3\"] = df[col].rolling(window=3).mean()\n",
    "\n",
    "# Time-based features\n",
    "df['month'] = df.index.month\n",
    "df['dayofyear'] = df.index.dayofyear\n",
    "\n",
    "# Drop rows with NaNs due to shifting/rolling\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "427939df-7a9f-47aa-9e29-aeb6993260c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: MULTI-STEP TARGET CREATION (t+1 to t+3)\n",
    "target_var = 'TMP'  # Target to forecast\n",
    "df['TMP_t+1'] = df[target_var].shift(-1)\n",
    "df['TMP_t+2'] = df[target_var].shift(-2)\n",
    "df['TMP_t+3'] = df[target_var].shift(-3)\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e07da73-112d-4547-b89b-9640b028a256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: TRAIN-TEST SPLIT & SCALING\n",
    "features = df.drop(columns=['TMP_t+1', 'TMP_t+2', 'TMP_t+3'])\n",
    "targets = df[['TMP_t+1', 'TMP_t+2', 'TMP_t+3']]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(features)\n",
    "\n",
    "train_size = int(len(df) * 0.8)\n",
    "X_train, X_test = X_scaled[:train_size], X_scaled[train_size:]\n",
    "y_train, y_test = targets[:train_size], targets[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d176b8c-39c8-401c-aa69-d7b94e563641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4: RANDOM FOREST WITH HYPERPARAMETER TUNING\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [10, None],\n",
    "    'min_samples_split': [2, 5],\n",
    "}\n",
    "# Define the model save directory\n",
    "model_dir = \"F:/projects/personal project/weather prediction_v1/models/\"\n",
    "\n",
    "best_models = {}\n",
    "\n",
    "for step in ['TMP_t+1', 'TMP_t+2', 'TMP_t+3']:\n",
    "    rf = RandomForestRegressor(random_state=42)\n",
    "    search = RandomizedSearchCV(rf, param_distributions=param_dist, n_iter=3, cv=2, n_jobs=-1)\n",
    "    search.fit(X_train, y_train[step])\n",
    "    best_models[step] = search.best_estimator_\n",
    "    model_path = os.path.join(model_dir, f\"rf_model_{step}.pkl\")\n",
    "    joblib.dump(best_models[step], model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aba44c57-7a9d-4b07-b313-aa230af958b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š TMP_t+1 Evaluation\n",
      "MAE: 0.4450822181090124\n",
      "RMSE: 0.7312935582267434\n",
      "R2 Score: 0.9995650732044311\n",
      "\n",
      "ğŸ“Š TMP_t+2 Evaluation\n",
      "MAE: 0.5487915975801441\n",
      "RMSE: 0.8121955301700466\n",
      "R2 Score: 0.9994635097197827\n",
      "\n",
      "ğŸ“Š TMP_t+3 Evaluation\n",
      "MAE: 0.5984560427480963\n",
      "RMSE: 0.8621495567024434\n",
      "R2 Score: 0.9993954929615083\n"
     ]
    }
   ],
   "source": [
    "# STEP 5: EVALUATION\n",
    "for step in ['TMP_t+1', 'TMP_t+2', 'TMP_t+3']:\n",
    "    model = best_models[step]\n",
    "    preds = model.predict(X_test)\n",
    "    print(f\"\\nğŸ“Š {step} Evaluation\")\n",
    "    print(\"MAE:\", mean_absolute_error(y_test[step], preds))\n",
    "    print(\"RMSE:\", np.sqrt(mean_squared_error(y_test[step], preds)))\n",
    "    print(\"R2 Score:\", r2_score(y_test[step], preds))\n",
    "\n",
    "    # Save prediction plots\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(y_test[step].values[:100], label='Actual')\n",
    "    plt.plot(preds[:100], label='Predicted')\n",
    "    plt.title(f\"{step} - Actual vs Predicted\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"prediction_plot_{step}.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75fe012a-a5e2-4311-9dc7-508c2183c213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“‰ Baseline Model (predict tomorrow = today)\n",
      "MAE: 251.60300377432955\n",
      "RMSE: 254.00514240939933\n"
     ]
    }
   ],
   "source": [
    "# STEP 6: BASELINE MODEL\n",
    "print(\"\\nğŸ“‰ Baseline Model (predict tomorrow = today)\")\n",
    "y_naive = X_test[:, list(features.columns).index('TMP')]  # today's TMP = tomorrow's forecast\n",
    "y_true = y_test['TMP_t+1'].values\n",
    "print(\"MAE:\", mean_absolute_error(y_true, y_naive))\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_true, y_naive)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "021385e3-c798-4c3d-b24f-8ae4fdef2589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 7: EXPLAINABILITY WITH SHAP (t+1 only)\n",
    "explainer = shap.TreeExplainer(best_models['TMP_t+1'])\n",
    "shap_values = explainer.shap_values(X_test[:100])\n",
    "shap.summary_plot(shap_values, features.columns, show=False)\n",
    "plt.savefig(\"F:/projects/personal project/weather prediction_v1/outputs/shap_summary_t+1.png\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cd991fee-0822-44a4-a1d3-0100c684ca25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Cross-validation (R2 scores)\n",
      "TMP_t+1: Mean R2 = 0.9996, Std = 0.0001\n"
     ]
    }
   ],
   "source": [
    "# STEP 8: CROSS-VALIDATION\n",
    "print(\"\\nğŸ“Š Cross-validation (R2 scores)\")\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "for step in ['TMP_t+1']:\n",
    "    scores = []\n",
    "    for train_idx, val_idx in tscv.split(X_scaled):\n",
    "        model = RandomForestRegressor()\n",
    "        model.fit(X_scaled[train_idx], targets[step].iloc[train_idx])\n",
    "        preds = model.predict(X_scaled[val_idx])\n",
    "        scores.append(r2_score(targets[step].iloc[val_idx], preds))\n",
    "    print(f\"{step}: Mean R2 = {np.mean(scores):.4f}, Std = {np.std(scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fba34155-90d1-4dea-99c6-901b2c54c6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Model Drift Analysis\n",
      "Early R2: 0.9999688765719816\n",
      "Late  R2: 0.9998039280927691\n"
     ]
    }
   ],
   "source": [
    "# STEP 9: MODEL DRIFT\n",
    "print(\"\\nğŸ“Š Model Drift Analysis\")\n",
    "early = slice(0, len(X_scaled)//2)\n",
    "late = slice(len(X_scaled)//2, None)\n",
    "model = best_models['TMP_t+1']\n",
    "print(\"Early R2:\", r2_score(targets['TMP_t+1'].iloc[early], model.predict(X_scaled[early])))\n",
    "print(\"Late  R2:\", r2_score(targets['TMP_t+1'].iloc[late], model.predict(X_scaled[late])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "689f55fd-e19b-47f5-a414-cd7416db5256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Project pipeline completed. Ready for dashboard!\n"
     ]
    }
   ],
   "source": [
    "# STEP 10: SAVE SCALER\n",
    "scaler_path = os.path.join(model_dir, \"scaler.pkl\")\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(\"\\nâœ… Project pipeline completed. Ready for dashboard!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369790e0-0c8c-4c5b-81bd-710646a7980b",
   "metadata": {},
   "source": [
    ">LSTM MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "96e5ad7a-3cc8-488f-a062-af976dfdfa8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m4600/4600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - loss: 16674.3496\n",
      "Epoch 2/20\n",
      "\u001b[1m4600/4600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 87.1544\n",
      "Epoch 3/20\n",
      "\u001b[1m4600/4600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 17.5639\n",
      "Epoch 4/20\n",
      "\u001b[1m4600/4600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 8.3863\n",
      "Epoch 5/20\n",
      "\u001b[1m4600/4600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 5.6620\n",
      "Epoch 6/20\n",
      "\u001b[1m4600/4600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 4.5847\n",
      "Epoch 7/20\n",
      "\u001b[1m4600/4600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 3.9958\n",
      "Epoch 8/20\n",
      "\u001b[1m4600/4600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 3.6845\n",
      "Epoch 9/20\n",
      "\u001b[1m4600/4600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 3.3156\n",
      "Epoch 10/20\n",
      "\u001b[1m4600/4600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 2.9127\n",
      "Epoch 11/20\n",
      "\u001b[1m4600/4600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 2.3010\n",
      "Epoch 12/20\n",
      "\u001b[1m4600/4600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 1.9156\n",
      "Epoch 13/20\n",
      "\u001b[1m4600/4600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 1.6014\n",
      "Epoch 14/20\n",
      "\u001b[1m4600/4600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 1.4901\n",
      "Epoch 15/20\n",
      "\u001b[1m4600/4600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 1.3503\n",
      "Epoch 16/20\n",
      "\u001b[1m4600/4600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 1.2674\n",
      "Epoch 17/20\n",
      "\u001b[1m4600/4600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1ms/step - loss: 1.2150\n",
      "Epoch 18/20\n",
      "\u001b[1m4600/4600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 1.1240\n",
      "Epoch 19/20\n",
      "\u001b[1m4600/4600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 1.0739\n",
      "Epoch 20/20\n",
      "\u001b[1m4600/4600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 1.0357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LSTM model saved: lstm_model_TMP_t+1.h5\n",
      "Epoch 1/20\n",
      "\u001b[1m4600/4600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - loss: 16544.0684\n",
      "Epoch 2/20\n",
      "\u001b[1m4600/4600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 77.9648\n",
      "Epoch 3/20\n",
      "\u001b[1m4600/4600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 28.6089\n",
      "Epoch 4/20\n",
      "\u001b[1m4600/4600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 9.7166\n",
      "Epoch 5/20\n",
      "\u001b[1m4600/4600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 3.8662\n",
      "Epoch 6/20\n",
      "\u001b[1m4600/4600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 2.7464\n",
      "Epoch 7/20\n",
      "\u001b[1m4600/4600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 2.1738\n",
      "Epoch 8/20\n",
      "\u001b[1m4600/4600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 1.8969\n",
      "Epoch 9/20\n",
      "\u001b[1m4600/4600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 1.7351\n",
      "Epoch 10/20\n",
      "\u001b[1m4600/4600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 1.5694\n",
      "Epoch 11/20\n",
      "\u001b[1m4600/4600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 1.5266\n",
      "Epoch 12/20\n",
      "\u001b[1m4600/4600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 1.3685\n",
      "Epoch 13/20\n",
      "\u001b[1m4600/4600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 1.3495\n",
      "Epoch 14/20\n",
      "\u001b[1m4600/4600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 1.1989\n",
      "Epoch 15/20\n",
      "\u001b[1m4600/4600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 1.2016\n",
      "Epoch 16/20\n",
      "\u001b[1m4600/4600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 1.0681\n",
      "Epoch 17/20\n",
      "\u001b[1m4600/4600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 1.0494\n",
      "Epoch 18/20\n",
      "\u001b[1m4600/4600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 0.9814\n",
      "Epoch 19/20\n",
      "\u001b[1m4600/4600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 0.9714\n",
      "Epoch 20/20\n",
      "\u001b[1m4600/4600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 0.9150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LSTM model saved: lstm_model_TMP_t+2.h5\n",
      "Epoch 1/20\n",
      "\u001b[1m4600/4600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - loss: 16006.2607\n",
      "Epoch 2/20\n",
      "\u001b[1m4600/4600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 79.7976\n",
      "Epoch 3/20\n",
      "\u001b[1m4600/4600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 8.2125\n",
      "Epoch 4/20\n",
      "\u001b[1m4600/4600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 3.3639\n",
      "Epoch 5/20\n",
      "\u001b[1m4600/4600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 2.6322\n",
      "Epoch 6/20\n",
      "\u001b[1m4600/4600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 2.1847\n",
      "Epoch 7/20\n",
      "\u001b[1m4600/4600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 1.9660\n",
      "Epoch 8/20\n",
      "\u001b[1m4600/4600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 1.7808\n",
      "Epoch 9/20\n",
      "\u001b[1m4600/4600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 1.7362\n",
      "Epoch 10/20\n",
      "\u001b[1m4600/4600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 1.6141\n",
      "Epoch 11/20\n",
      "\u001b[1m4600/4600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 1.4735\n",
      "Epoch 12/20\n",
      "\u001b[1m4600/4600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - loss: 1.3639\n",
      "Epoch 13/20\n",
      "\u001b[1m4600/4600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 1.3629\n",
      "Epoch 14/20\n",
      "\u001b[1m4600/4600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - loss: 1.2860\n",
      "Epoch 15/20\n",
      "\u001b[1m4600/4600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - loss: 1.2427\n",
      "Epoch 16/20\n",
      "\u001b[1m4600/4600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 1.2216\n",
      "Epoch 17/20\n",
      "\u001b[1m4600/4600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - loss: 1.1172\n",
      "Epoch 18/20\n",
      "\u001b[1m4600/4600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 1.0856\n",
      "Epoch 19/20\n",
      "\u001b[1m4600/4600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 1.1039\n",
      "Epoch 20/20\n",
      "\u001b[1m4600/4600\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1ms/step - loss: 1.0995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LSTM model saved: lstm_model_TMP_t+3.h5\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "\n",
    "scaler = joblib.load(os.path.join(model_dir, \"scaler.pkl\"))\n",
    "X_scaled = scaler.transform(features)\n",
    "y = targets\n",
    "# Reshape X for LSTM: (samples, timesteps=1, features)\n",
    "X_lstm = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
    "\n",
    "# Define function to train and save LSTM model\n",
    "def train_and_save_lstm(y_data, label):\n",
    "    model = Sequential([\n",
    "        LSTM(64, input_shape=(1, X_lstm.shape[2]), activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    model.fit(X_lstm, y_data, epochs=20, batch_size=32, verbose=1)\n",
    "    model_path = os.path.join(model_dir, f\"lstm_model_{label}.h5\")\n",
    "    model.save(model_path)\n",
    "    print(f\"âœ… LSTM model saved: lstm_model_{label}.h5\")\n",
    "\n",
    "# Train models for t+1, t+2, t+3\n",
    "train_and_save_lstm(y['TMP_t+1'], 'TMP_t+1')\n",
    "train_and_save_lstm(y['TMP_t+2'], 'TMP_t+2')\n",
    "train_and_save_lstm(y['TMP_t+3'], 'TMP_t+3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2da992f-81de-49bd-a436-f202fa8f784e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bcac35-2e8e-431f-8ddc-381026974dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6d6063-2657-462d-a1bd-8938c42deb1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
